{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bb410a5-9e3b-4415-904e-43da53857408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19853bfa-1056-40b7-b5d8-bf6d580d287e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/titanic/titanic_training.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m titanic_data_train \u001b[38;5;241m=\u001b[39m \u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatasets/titanic/titanic_training.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m titanic_data_test \u001b[38;5;241m=\u001b[39m read_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/titanic/titanic_testing_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/titanic/titanic_training.csv'"
     ]
    }
   ],
   "source": [
    "titanic_data_train = read_csv('datasets/titanic/titanic_training.csv')\n",
    "titanic_data_test = read_csv('datasets/titanic/titanic_testing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e2ffbf2-f338-4c44-ac4f-052df5f5b57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\n",
      "  Downloading pydot-3.0.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pyparsing>=3.0.9 in /home/seraphima-kassianik/.config/jupyterlab-desktop/jlab_server/lib/python3.12/site-packages (from pydot) (3.1.4)\n",
      "Downloading pydot-3.0.2-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-3.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603a650e-36dd-45f3-bd49-465d621902b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d51dbce2-f37e-4c0b-afcf-55a1afae5311",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m genfromtxt\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier, export_graphviz\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, ClassifierMixin\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_val_score\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import scipy.io\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from pydot import graph_from_dot_data\n",
    "import io\n",
    "\n",
    "import random\n",
    "random.seed(246810)\n",
    "np.random.seed(246810)\n",
    "\n",
    "eps = 1e-5  # a small number\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, max_depth=3, feature_labels=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.features = feature_labels\n",
    "        self.left, self.right = None, None  # for non-leaf nodes\n",
    "        self.split_idx, self.thresh = None, None  # for non-leaf nodes\n",
    "        self.data, self.pred = None, None  # for leaf nodes\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(y):\n",
    "        label_counts = Counter(y)\n",
    "        probabilities = [count / len(y) for count in label_counts.values()]\n",
    "        return -sum(p * np.log2(p) for p in probabilities if p > 0)\n",
    "        # pass\n",
    "\n",
    "    @staticmethod\n",
    "    def information_gain(X, y, thresh):\n",
    "        # TODO\n",
    "        entropy_before = entropy(y)\n",
    "        X_left, y_left, X_right, y_right = self.split(X, y, idx, thresh)\n",
    "        entropy_after = (len(y_left) / len(y) * entropy(y_left) + \n",
    "                         len(y_right) / len(y) * entropy(y_right))\n",
    "        return entropy_before - entropy_after\n",
    "\n",
    "        # return np.random.rand()\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_impurity(X, y, thresh):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_purification(X, y, thresh):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def split(self, X, y, idx, thresh):\n",
    "        X0, idx0, X1, idx1 = self.split_test(X, idx=idx, thresh=thresh)\n",
    "        y0, y1 = y[idx0], y[idx1]\n",
    "        return X0, y0, X1, y1\n",
    "\n",
    "    def split_test(self, X, idx, thresh):\n",
    "        idx0 = np.where(X[:, idx] < thresh)[0]\n",
    "        idx1 = np.where(X[:, idx] >= thresh)[0]\n",
    "        X0, X1 = X[idx0, :], X[idx1, :]\n",
    "        return X0, idx0, X1, idx1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # TODO\n",
    "        self.root = self._fit_tree(X, y, depth=0)\n",
    "        #pass\n",
    "    def _fit_tree(self, X, y, depth):\n",
    "        if depth >= self.max_depth or len(np.unique(y)) == 1:\n",
    "            return {\"is_leaf\": True, \"prediction\": Counter(y).most_common(1)[0][0]}\n",
    "\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_thresh = None\n",
    "        for idx in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, idx])\n",
    "            for thresh in thresholds:\n",
    "                gain = self.information_gain(X, y, idx, thresh)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature, best_thresh = idx, thresh\n",
    "\n",
    "        if best_gain == 0:\n",
    "            return {\"is_leaf\": True, \"prediction\": Counter(y).most_common(1)[0][0]}\n",
    "\n",
    "        X_left, y_left, X_right, y_right = self.split(X, y, best_feature, best_thresh)\n",
    "\n",
    "        left_subtree = self._fit_tree(X_left, y_left, depth + 1)\n",
    "        right_subtree = self._fit_tree(X_right, y_right, depth + 1)\n",
    "\n",
    "        return {\"is_leaf\": False, \"split_feature\": best_feature, \n",
    "                \"split_thresh\": best_thresh, \"left\": left_subtree, \n",
    "                \"right\": right_subtree}\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO\n",
    "        #pass\n",
    "        return np.array([self._predict_sample(sample, self.tree) for sample in X])\n",
    "    def _predict_sample(self, sample, node):\n",
    "        if node[\"is_leaf\"]:\n",
    "            return node[\"prediction\"]\n",
    "        if sample[node[\"split_feature\"]] < node[\"split_thresh\"]:\n",
    "            return self._predict_sample(sample, node[\"left\"])\n",
    "        else:\n",
    "            return self._predict_sample(sample, node[\"right\"])\n",
    "\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.max_depth == 0:\n",
    "            return \"%s (%s)\" % (self.pred, self.labels.size)\n",
    "        else:\n",
    "            return \"[%s < %s: %s | %s]\" % (self.features[self.split_idx],\n",
    "                                           self.thresh, self.left.__repr__(),\n",
    "                                           self.right.__repr__())\n",
    "\n",
    "\n",
    "class BaggedTrees(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, params=None, n=200):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        self.params = params\n",
    "        self.n = n\n",
    "        self.decision_trees = [\n",
    "            DecisionTreeClassifier(random_state=i, **self.params)\n",
    "            for i in range(self.n)\n",
    "        ]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # TODO\n",
    "        # pass\n",
    "        for tree in self.decision_trees:\n",
    "            indices = np.random.randint(0, len(X), len(X))\n",
    "            tree.fit(X[indices], y[indices])\n",
    "\n",
    "    def predict(self, X):\n",
    "        # TODO\n",
    "        # pass\n",
    "        predictions = np.array([tree.predict(X) for tree in self.decision_trees])\n",
    "        return np.apply_along_axis(lambda x: np.bincount(x, minlength=2).argmax(), axis=0, arr=predictions)\n",
    "\n",
    "\n",
    "\n",
    "class RandomForest(BaggedTrees):\n",
    "\n",
    "    def __init__(self, params=None, n=200, m=1):\n",
    "        if params is None:\n",
    "            params = {}\n",
    "        params['max_features'] = m\n",
    "        self.m = m\n",
    "        super().__init__(params=params, n=n)\n",
    "\n",
    "\n",
    "class BoostedRandomForest(RandomForest):\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # TODO\n",
    "        #pass\n",
    "        sample_weights = np.ones(len(X)) / len(X)\n",
    "        for _ in range(self.num_boosting_rounds):\n",
    "            forest = RandomForest(params=self.params, n=self.n, m=self.m)\n",
    "            forest.fit(X, y)\n",
    "        self.boosted_forests.append(forest)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # TODO\n",
    "        # pass\n",
    "        predictions = np.array([forest.predict(X) for forest in self.boosted_forests])\n",
    "        final_predictions = np.apply_along_axis(lambda x: np.bincount(x, minlength=2).argmax(), axis=0, arr=predictions)\n",
    "        return final_predictions\n",
    "\n",
    "def preprocess(data, fill_mode=True, min_freq=10, onehot_cols=[]):\n",
    "    # fill_mode = False\n",
    "\n",
    "    # Temporarily assign -1 to missing data\n",
    "    data[data == b''] = '-1'\n",
    "\n",
    "    # Hash the columns (used for handling strings)\n",
    "    onehot_encoding = []\n",
    "    onehot_features = []\n",
    "    for col in onehot_cols:\n",
    "        counter = Counter(data[:, col])\n",
    "        for term in counter.most_common():\n",
    "            if term[0] == b'-1':\n",
    "                continue\n",
    "            if term[-1] <= min_freq:\n",
    "                break\n",
    "            onehot_features.append(term[0])\n",
    "            onehot_encoding.append((data[:, col] == term[0]).astype(float))\n",
    "        data[:, col] = '0'\n",
    "    onehot_encoding = np.array(onehot_encoding).T\n",
    "    data = np.hstack(\n",
    "        [np.array(data, dtype=float),\n",
    "         np.array(onehot_encoding)])\n",
    "\n",
    "    # Replace missing data with the mode value. We use the mode instead of\n",
    "    # the mean or median because this makes more sense for categorical\n",
    "    # features such as gender or cabin type, which are not ordered.\n",
    "    if fill_mode:\n",
    "        # TODO\n",
    "        # pass\n",
    "        for col in range(data.shape[1]):\n",
    "            col_data = list(data[:, col])\n",
    "            most_common = Counter(col_data).most_common(1)[0][0]\n",
    "            data[:, col] = [x if x != -1 else most_common for x in col_data]\n",
    "\n",
    "    return data, onehot_features\n",
    "\n",
    "\n",
    "def evaluate(clf):\n",
    "    print(\"Cross validation\", cross_val_score(clf, X, y))\n",
    "    if hasattr(clf, \"decision_trees\"):\n",
    "        counter = Counter([t.tree_.feature[0] for t in clf.decision_trees])\n",
    "        first_splits = [\n",
    "            (features[term[0]], term[1]) for term in counter.most_common()\n",
    "        ]\n",
    "        print(\"First splits\", first_splits)\n",
    "def evaluate_classifier(clf, X, y, features):\n",
    "    evaluate(clf)\n",
    "    \n",
    "    print(\"Cross-validation scores:\", cross_val_score(clf, X, y, cv=5))\n",
    "    \n",
    "    if hasattr(clf, \"decision_trees\"):\n",
    "        counter = Counter([tree.tree_.feature[0] for tree in clf.decision_trees])\n",
    "        first_splits = [(features[term[0]], term[1]) for term in counter.most_common()]\n",
    "        print(\"First splits using clf.decision_trees:\", first_splits)\n",
    "    else:\n",
    "        if hasattr(clf, 'tree_'):\n",
    "            first_split_feature_index = clf.tree_.feature[0]\n",
    "            if first_split_feature_index != -2: \n",
    "                print(\"First split feature using clf.tree_:\", features[first_split_feature_index])\n",
    "            else:\n",
    "                print(\"The tree is a stump.\")\n",
    "        else:\n",
    "            print(\"Classifier does not have tree_ attribute. Cannot print first split feature.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = \"titanic\"\n",
    "    # dataset = \"spam\"\n",
    "    params = {\n",
    "        \"max_depth\": 5,\n",
    "        # \"random_state\": 6,\n",
    "        \"min_samples_leaf\": 10,\n",
    "    }\n",
    "    N = 100\n",
    "\n",
    "    if dataset == \"titanic\":\n",
    "        # Load titanic data\n",
    "        path_train = 'datasets/titanic/titanic_training.csv'\n",
    "        data = genfromtxt(path_train, delimiter=',', dtype=None)\n",
    "        path_test = 'datasets/titanic/titanic_testing_data.csv'\n",
    "        test_data = genfromtxt(path_test, delimiter=',', dtype=None)\n",
    "        y = data[1:, 0]  # label = survived\n",
    "        class_names = [\"Died\", \"Survived\"]\n",
    "\n",
    "        labeled_idx = np.where(y != b'')[0]\n",
    "        y = np.array(y[labeled_idx], dtype=float).astype(int)\n",
    "        print(\"\\n\\nPart (b): preprocessing the titanic dataset\")\n",
    "        X, onehot_features = preprocess(data[1:, 1:], onehot_cols=[1, 5, 7, 8])\n",
    "        X = X[labeled_idx, :]\n",
    "        Z, _ = preprocess(test_data[1:, :], onehot_cols=[1, 5, 7, 8])\n",
    "        assert X.shape[1] == Z.shape[1]\n",
    "        features = list(data[0, 1:]) + onehot_features\n",
    "\n",
    "    elif dataset == \"spam\":\n",
    "        features = [\n",
    "            \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\",\n",
    "            \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\",\n",
    "            \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\",\n",
    "            \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\",\n",
    "            \"square_bracket\", \"ampersand\"\n",
    "        ]\n",
    "        assert len(features) == 32\n",
    "\n",
    "        # Load spam data\n",
    "        path_train = 'datasets/spam_data/spam_data.mat'\n",
    "        data = scipy.io.loadmat(path_train)\n",
    "        X = data['training_data']\n",
    "        y = np.squeeze(data['training_labels'])\n",
    "        Z = data['test_data']\n",
    "        class_names = [\"Ham\", \"Spam\"]\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Dataset %s not handled\" % dataset)\n",
    "\n",
    "    print(\"Features\", features)\n",
    "    print(\"Train/test size\", X.shape, Z.shape)\n",
    "    \n",
    "    print(\"\\n\\nPart 0: constant classifier\")\n",
    "    print(\"Accuracy\", 1 - np.sum(y) / y.size)\n",
    "\n",
    "    # sklearn decision tree\n",
    "    print(\"\\n\\nsklearn's decision tree\")\n",
    "    clf = DecisionTreeClassifier(random_state=0, **params)\n",
    "    clf.fit(X, y)\n",
    "    evaluate(clf)\n",
    "    out = io.StringIO()\n",
    "    export_graphviz(\n",
    "        clf, out_file=out, feature_names=features, class_names=class_names)\n",
    "    # For OSX, may need the following for dot: brew install gprof2dot\n",
    "    graph = graph_from_dot_data(out.getvalue())\n",
    "    graph_from_dot_data(out.getvalue())[0].write_pdf(\"%s-tree.pdf\" % dataset)\n",
    "    \n",
    "    # TODO\n",
    "    print(\"\\n\\nsklearn's decision tree\")\n",
    "    clf = DecisionTreeClassifier(random_state=0, **params)\n",
    "    clf.fit(X, y)\n",
    "    evaluate_classifier(clf, X, y, features)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63243315-170b-484a-84ae-4f614a61a176",
   "metadata": {},
   "source": [
    "<font size=\"+2\">Titanic</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bc3afa0-34fa-4816-8561-59099d5edbfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>359309</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>female</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35273</td>\n",
       "      <td>113.2750</td>\n",
       "      <td>D36</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SOTON/O.Q. 3101306</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SC/A.3 2861</td>\n",
       "      <td>15.5792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>349227</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch              ticket      fare  \\\n",
       "0       0.0     3.0    male   NaN    0.0    0.0              359309    8.0500   \n",
       "1       1.0     1.0  female  23.0    1.0    0.0               35273  113.2750   \n",
       "2       0.0     3.0     NaN  38.0    0.0    0.0  SOTON/O.Q. 3101306    7.0500   \n",
       "3       0.0     2.0    male   NaN    0.0    0.0         SC/A.3 2861   15.5792   \n",
       "4       0.0     3.0     NaN   NaN    0.0    0.0              349227    7.8958   \n",
       "\n",
       "  cabin embarked  \n",
       "0   NaN        S  \n",
       "1   D36        C  \n",
       "2   NaN        S  \n",
       "3   NaN        C  \n",
       "4   NaN        S  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87c97c32-273a-4843-ad50-30c79e706617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "Training Accuracy: 0.9739776951672863, Validation Accuracy: 0.7673267326732673\n",
      "Random Forest:\n",
      "Training Accuracy: 0.9739776951672863, Validation Accuracy: 0.801980198019802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6381/1208791481.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['IsAlone'].loc[data['FamilySize'] > 1] = 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def preprocess_data(data):\n",
    "    data['FamilySize'] = data['sibsp'] + data['parch'] + 1\n",
    "    \n",
    "    data['IsAlone'] = 1 \n",
    "    data['IsAlone'].loc[data['FamilySize'] > 1] = 0 \n",
    "    \n",
    "    data['FarePerPerson'] = data['fare'] / data['FamilySize']\n",
    "    \n",
    "    data = data.drop(['sibsp', 'parch', 'ticket'], axis=1)\n",
    "\n",
    "    numerical_features = ['age', 'fare']\n",
    "    categorical_features = ['sex', 'embarked']\n",
    "    \n",
    "    numerical_transformer = SimpleImputer(strategy='median')\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    X = data.drop('survived', axis=1)\n",
    "    y = data['survived'].copy()\n",
    "    \n",
    "    X_transformed = preprocessor.fit_transform(X)\n",
    "    y = y.astype(int)  \n",
    "    \n",
    "    return X_transformed, y, preprocessor\n",
    "\n",
    "X, y, preprocessor = preprocess_data(titanic_data_train)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "def evaluate_model(clf, X_train, X_val, y_train, y_val):\n",
    "    train_preds = clf.predict(X_train)\n",
    "    val_preds = clf.predict(X_val)\n",
    "    train_acc = accuracy_score(y_train, train_preds)\n",
    "    val_acc = accuracy_score(y_val, val_preds)\n",
    "    print(f\"Training Accuracy: {train_acc}, Validation Accuracy: {val_acc}\")\n",
    "\n",
    "print(\"Decision Tree:\")\n",
    "evaluate_model(dt_clf, X_train, X_val, y_train, y_val)\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "evaluate_model(rf_clf, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2e83dcd5-e3f6-4a71-ab28-5093680c15df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Random Forest: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],  \n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]  \n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(estimator=RandomForestClassifier(random_state=42), \n",
    "                              param_grid=param_grid_rf, \n",
    "                              cv=5, \n",
    "                              scoring='accuracy', \n",
    "                              n_jobs=-1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters for Random Forest:\", grid_search_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aa9edb5c-64a8-4197-9537-d0038464475f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Random Forest:\n",
      "Training Accuracy: 0.8847583643122676, Validation Accuracy: 0.8267326732673267\n"
     ]
    }
   ],
   "source": [
    "rf_clf_optimized = grid_search_rf.best_estimator_\n",
    "# print(\"Optimized Decision Tree:\")\n",
    "# evaluate_model(dt_clf_optimized, X_train, X_val, y_train, y_val)\n",
    "\n",
    "print(\"Optimized Random Forest:\")\n",
    "evaluate_model(rf_clf_optimized, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "636833c4-d2aa-4fcc-af3f-1a32ca996499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of Optimized Model: 0.8267326732673267\n"
     ]
    }
   ],
   "source": [
    "rf_optimized = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=10,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_optimized.fit(X_train, y_train)\n",
    "\n",
    "val_preds = rf_optimized.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_preds)\n",
    "print(f\"Validation Accuracy of Optimized Model: {val_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5c2b94f-8eeb-4f0c-b314-58f268a827cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "X_test_transformed = preprocessor.transform(titanic_data_test)\n",
    "predictions_titanic = rf_optimized.predict(X_test_transformed)\n",
    "def results_to_csv(y_test):\n",
    "    y_test = y_test.astype(int)\n",
    "    df = pd.DataFrame({'Category': y_test})\n",
    "    df.index += 1 # Ensures that the index starts at 1\n",
    "    df.to_csv('submission_rf_titanic.csv', index_label='Id')\n",
    "results_to_csv(predictions_titanic)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da158f3d-bf6c-49fb-a152-90f934038e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "X_test_transformed = preprocessor.transform(titanic_data_test)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6eb0e51c-1e7c-4329-96ae-525381b3f434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "predictions_rf = rf_clf.predict(X_test_transformed)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8584b12e-9274-4731-b630-87b2d58cb494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "predictions_dt = dt_clf.predict(X_test_transformed)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "04223ab8-94b6-4381-813b-0fbc1b2c7035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_csv(y_test):\n",
    "    y_test = y_test.astype(int)\n",
    "    df = pd.DataFrame({'Category': y_test})\n",
    "    df.index += 1 # Ensures that the index starts at 1\n",
    "    df.to_csv('submission_decision_tree.csv', index_label='Id')\n",
    "results_to_csv(predictions_dt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad2bd5b3-9ecd-4375-8ea0-ac2b8ed08230",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_data = scipy.io.loadmat('datasets/spam_data/spam_data.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62807f5a-c43a-4321-bc70-6b703091dd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Sun Mar  3 21:01:43 2024',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'training_data': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 2., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 1., 0., 0.]]),\n",
       " 'test_data': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 2., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 3.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'training_labels': array([[1., 0., 0., ..., 1., 1., 0.]])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f4c149e-e24e-42ea-89eb-c747f1d06088",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = spam_data['test_data']\n",
    "\n",
    "training_labels = spam_data['training_labels'].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc92c831-acdc-4b3b-931d-290db75965e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "            \"pain\", \"private\", \"bank\", \"money\", \"drug\", \"spam\", \"prescription\",\n",
    "            \"creative\", \"height\", \"featured\", \"differ\", \"width\", \"other\",\n",
    "            \"energy\", \"business\", \"message\", \"volumes\", \"revision\", \"path\",\n",
    "            \"meter\", \"memo\", \"planning\", \"pleased\", \"record\", \"out\",\n",
    "            \"semicolon\", \"dollar\", \"sharp\", \"exclamation\", \"parenthesis\",\n",
    "            \"square_bracket\", \"ampersand\"\n",
    "        ]\n",
    "assert len(features) == 32\n",
    "\n",
    "# Load spam data\n",
    "path_train = 'datasets/spam_data/spam_data.mat'\n",
    "data = scipy.io.loadmat(path_train)\n",
    "X = data['training_data']\n",
    "y = np.squeeze(data['training_labels'])\n",
    "Z = data['test_data']\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bc89282-b5ef-44bb-8369-f95b3fad1037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.io\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2177015-e781-4962-be65-7d2d3aa4d706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "Model: DecisionTreeClassifier\n",
      "Training Accuracy: 0.9011769931157007\n",
      "Validation Accuracy: 0.80550621669627\n",
      "\n",
      "Random Forest:\n",
      "Model: RandomForestClassifier\n",
      "Training Accuracy: 0.9011769931157007\n",
      "Validation Accuracy: 0.8259325044404974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "def evaluate_model(clf, X_train, X_val, y_train, y_val):\n",
    "    train_preds = clf.predict(X_train)\n",
    "    val_preds = clf.predict(X_val)\n",
    "    train_acc = accuracy_score(y_train, train_preds)\n",
    "    val_acc = accuracy_score(y_val, val_preds)\n",
    "    print(f\"Model: {clf.__class__.__name__}\\nTraining Accuracy: {train_acc}\\nValidation Accuracy: {val_acc}\\n\")\n",
    "\n",
    "print(\"Decision Tree:\")\n",
    "evaluate_model(dt_clf, X_train, X_val, y_train, y_val)\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "evaluate_model(rf_clf, X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fad8a1d2-49b0-4726-a331-7e1722976e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions from Random Forest done.\n",
      "Predictions from Decision Tree done.\n"
     ]
    }
   ],
   "source": [
    "predictions_rf_spam = rf_clf.predict(Z)\n",
    "print(\"Predictions from Random Forest done.\")\n",
    "\n",
    "predictions_dt_spam = dt_clf.predict(Z)\n",
    "print(\"Predictions from Decision Tree done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7ed0d85-9889-40db-bf5a-7edcf7bc4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_csv(y_test):\n",
    "    y_test = y_test.astype(int)\n",
    "    df = pd.DataFrame({'Category': y_test})\n",
    "    df.index += 1 # Ensures that the index starts at 1\n",
    "    df.to_csv('submission_spam_decsion_tree.csv', index_label='Id')\n",
    "results_to_csv(predictions_dt_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7eb8d4-3114-4d84-8661-a118983b39a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
